{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyTamila75/DeFi_PanCakeSwapBot/blob/master/flux_1_kontext_dev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjYy0F2gZIPR"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/flux1-dev-fp8.safetensors -d /content/TotoroUI/models/unet -o flux1-dev-fp8.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro import model_management\n",
        "\n",
        "# Re-instantiate nodes (assuming the previous cells for cloning and installing have been run)\n",
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]()\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]()\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
        "VAEEncode = NODE_CLASS_MAPPINGS[\"VAEEncode\"]() # We need VAEEncode for image-to-image\n",
        "LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]() # We need LoadImage to load our source image\n",
        "\n",
        "# Define the closestNumber helper function (from your previous code)\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2\n",
        "\n",
        "with torch.inference_mode():\n",
        "    # Ensure models are loaded (re-run this block if your Colab session restarted)\n",
        "    # These lines assume the model files were downloaded to the correct paths in previous cells\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"flux1-dev-fp8.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "    # --- IMAGE EDITING PARAMETERS ---\n",
        "    # Path to your input image. Make sure this image exists in your Colab environment\n",
        "    # or in your mounted Google Drive (e.g., \"/content/drive/MyDrive/my_image.png\")\n",
        "    input_image_path = \"/content/test_image.png\" # <--- IMPORTANT: Change this to your image path!\n",
        "\n",
        "    # Create a placeholder image if it doesn't exist for demonstration\n",
        "    # In a real scenario, you'd upload your own image.\n",
        "    try:\n",
        "        Image.open(input_image_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Input image not found at {input_image_path}. Creating a placeholder.\")\n",
        "        dummy_image = Image.new('RGB', (1024, 1024), color = 'red')\n",
        "        dummy_image.save(input_image_path)\n",
        "        print(f\"Placeholder image saved to {input_image_path}\")\n",
        "\n",
        "    # The text instruction for editing the image\n",
        "    positive_prompt = \"Change the red background to a lush green forest, add a small, cute brown dog in the bottom left corner, keep the main subject in the center.\"\n",
        "    # You can add a negative prompt to guide what you DON'T want\n",
        "    negative_prompt = \"blurry, distorted, ugly, bad anatomy, deformed, extra limbs, watermark, text, signature\"\n",
        "\n",
        "    # Denoise strength: 1.0 means completely ignore the original image (text-to-image from noise)\n",
        "    # A value between 0.5 and 0.8 is typical for image editing, preserving some of the original structure.\n",
        "    # Lower values (e.g., 0.2-0.4) for subtle changes, higher values (e.g., 0.6-0.8) for more significant transformations.\n",
        "    denoise_strength = 0.7 # <--- Adjust this value to control how much the image changes!\n",
        "\n",
        "    # Other parameters (can be adjusted)\n",
        "    seed = 0 # Set to 0 for random seed, or a specific number for reproducibility\n",
        "    steps = 25 # More steps can improve quality, but take longer\n",
        "    sampler_name = \"euler\"\n",
        "    scheduler = \"simple\"\n",
        "\n",
        "    # Generate a random seed if set to 0\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(f\"Using seed: {seed}\")\n",
        "\n",
        "    # --- Image-to-Image Workflow ---\n",
        "    # 1. Load the input image\n",
        "    # LoadImage.load_image returns a tuple (image_tensor, image_width, image_height, image_channels, image_mode)\n",
        "    # We only need the image_tensor for VAE encoding.\n",
        "    input_image_tensor = LoadImage.load_image(input_image_path)[0]\n",
        "\n",
        "    # 2. Encode the input image into latent space\n",
        "    # The VAEEncode node expects a pixel tensor (input_image_tensor) and the VAE model.\n",
        "    # It returns a latent representation.\n",
        "    initial_latent = VAEEncode.encode(vae, input_image_tensor)[0]\n",
        "\n",
        "    # 3. Prepare conditioning (positive and negative prompts)\n",
        "    cond_pos, pooled_pos = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "    cond_pos = [[cond_pos, {\"pooled_output\": pooled_pos}]]\n",
        "\n",
        "    cond_neg, pooled_neg = clip.encode_from_tokens(clip.tokenize(negative_prompt), return_pooled=True)\n",
        "    cond_neg = [[cond_neg, {\"pooled_output\": pooled_neg}]]\n",
        "\n",
        "    # 4. Generate noise for the sampler\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "\n",
        "    # 5. Prepare guider, sampler, and sigmas\n",
        "    guider = BasicGuider.get_guider(unet, cond_pos)[0] # Guider uses positive conditioning\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    # Sigmas for image-to-image start from a higher noise level (denoise_strength)\n",
        "    sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, denoise_strength)[0]\n",
        "\n",
        "    # 6. Run the sampler with the initial_latent (encoded input image)\n",
        "    # The SamplerCustomAdvanced takes the noise, guider, sampler, sigmas, and the initial_latent\n",
        "    # It will denoise the initial_latent towards the prompt, guided by the noise schedule.\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(\n",
        "        noise, guider, sampler, sigmas, initial_latent,\n",
        "        negative_cond=cond_neg # Pass negative conditioning here\n",
        "    )\n",
        "\n",
        "    # 7. Clear memory and decode the latent image back to pixels\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded_image_tensor = VAEDecode.decode(vae, sample)[0].detach()\n",
        "\n",
        "    # 8. Convert to PIL Image and save\n",
        "    # ComfyUI's decoded images are typically in the range [0, 1] and in (batch, height, width, channels) format\n",
        "    # PIL expects (height, width, channels) and values 0-255 (uint8)\n",
        "    output_image_array = np.array(decoded_image_tensor * 255, dtype=np.uint8)[0]\n",
        "    output_pil_image = Image.fromarray(output_image_array)\n",
        "\n",
        "    # Save the output image\n",
        "    output_image_path = \"/content/edited_flux_image.png\"\n",
        "    output_pil_image.save(output_image_path)\n",
        "    print(f\"Edited image saved to: {output_image_path}\")\n",
        "\n",
        "# Display the generated image in the Colab output\n",
        "output_pil_image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT_DY1E87UOA"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    positive_prompt = \"black forest toast spelling out the words 'FLUX DEV', tasty, food photography, dynamic shot\"\n",
        "    width = 1024\n",
        "    height = 1024\n",
        "    seed = 0\n",
        "    steps = 20\n",
        "    sampler_name = \"euler\"\n",
        "    scheduler = \"simple\"\n",
        "\n",
        "    if seed == 0:\n",
        "        seed = random.randint(0, 18446744073709551615)\n",
        "    print(seed)\n",
        "\n",
        "    cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "    cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "    noise = RandomNoise.get_noise(seed)[0]\n",
        "    guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "    sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 1.0)[0]\n",
        "    latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "    sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "    model_management.soft_empty_cache()\n",
        "    decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(\"/content/flux.png\")\n",
        "\n",
        "Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}